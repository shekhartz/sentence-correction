{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mG_1reu7uM74"
   },
   "source": [
    "# Final Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2-EJqakEV8z"
   },
   "source": [
    "## Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C8c3NwDGZSdh"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk.translate.bleu_score as bleu\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Softmax, RNN, Dense, Embedding, LSTM, Flatten, Activation, Bidirectional, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VWejJV0H_sLd",
    "outputId": "ab0f2f30-e68a-4551-cc38-f8c69629f377"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZjnALK_l5Vpe"
   },
   "outputs": [],
   "source": [
    "model_path = '/content/drive/MyDrive/CS2/3.Models/4_5_Attention-Bi-Word-Fasttext/'\n",
    "final_path = '/content/drive/MyDrive/CS2/4.Final/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NCL5fOC6C28O"
   },
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W1ZycZ1xT_E1"
   },
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text = re.sub('<.*>', '', text)\n",
    "    text = re.sub('\\(.*\\)', '', text)\n",
    "    text = re.sub('\\[.*\\]', '', text)\n",
    "    text = re.sub('{.*}', '', text)\n",
    "    text = re.sub(\"[-+@#^/|*(){}$~`<>=_]\",\"\",text)\n",
    "    text = text.replace(\"\\\\\",\"\")\n",
    "    text = re.sub(\"\\[\",\"\",text)\n",
    "    text = re.sub(\"\\]\",\"\",text)\n",
    "    text = re.sub(\"[0-9]\",\"\",text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uiR00g4hDEtl"
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zbIRAOoUQ9mY"
   },
   "outputs": [],
   "source": [
    "with open(final_path+'tokenizer_i.pickle', 'rb') as handle:\n",
    "    tokenizer_i = pickle.load(handle)\n",
    "\n",
    "with open(final_path+'tokenizer_o.pickle', 'rb') as handle:\n",
    "    tokenizer_o = pickle.load(handle)\n",
    "\n",
    "input_vocab = tokenizer_i.word_index\n",
    "output_vocab = tokenizer_o.word_index\n",
    "\n",
    "vocab_size_input = len(tokenizer_i.word_index.keys())\n",
    "vocab_size_output = len(tokenizer_o.word_index.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFL5Yw4ndN9e"
   },
   "source": [
    "## Fasttext Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4pkEgtPmUuDj"
   },
   "outputs": [],
   "source": [
    "encoder_embedding_matrix_fast = np.load(final_path + 'encoder_embedding_matrix_fast.npy')\n",
    "decoder_embedding_matrix_fast = np.load(final_path + 'decoder_embedding_matrix_fast.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1RHXoozdDrug"
   },
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aCH8qtWgDFcz"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    '''\n",
    "    Encoder model -- That takes a input sequence and returns encoder-outputs,encoder_final_state_h,encoder_final_state_c\n",
    "    '''\n",
    "    def __init__(self,in_vocab_size,embedding_dim,enc_units,input_length):\n",
    "        super().__init__()\n",
    "        self.in_vocab_size = in_vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.input_length = input_length\n",
    "        self.enc_units = enc_units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.embedding = Embedding(input_dim=self.in_vocab_size, output_dim=self.embedding_dim, input_length=self.input_length, mask_zero=True, weights=[encoder_embedding_matrix_fast], trainable=False, name=\"Encoder_Embedding\")\n",
    "        self.lstm = Bidirectional(LSTM(self.enc_units, return_state=True, return_sequences=True, name=\"Encoder_LSTM\"))\n",
    "        \n",
    "    def call(self, input_sentences, training=True):\n",
    "        input_embed = self.embedding(input_sentences)\n",
    "        encoder_output, encoder_state_h_fwd, encoder_state_c_fwd, encoder_state_h_bwd, encoder_state_c_bwd = self.lstm(input_embed)\n",
    "        encoder_state_h = Concatenate()([encoder_state_h_fwd, encoder_state_h_bwd])\n",
    "        encoder_state_c = Concatenate()([encoder_state_c_fwd, encoder_state_c_bwd])\n",
    "        return encoder_output, encoder_state_h, encoder_state_c\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------\n",
    "class Attention(tf.keras.layers.Layer):\n",
    "  '''\n",
    "  Class the calculates score based on the scoring_function using Bahdanu attention mechanism.\n",
    "  '''\n",
    "  def __init__(self,scoring_function, att_units):\n",
    "    super().__init__()\n",
    "    self.scoring_function = scoring_function\n",
    "    self.att_units = att_units\n",
    "\n",
    "    if self.scoring_function == 'dot':\n",
    "      pass\n",
    "\n",
    "    if scoring_function == 'general':\n",
    "      self.W = Dense(self.att_units)\n",
    "      \n",
    "    elif scoring_function == 'concat':\n",
    "      self.W1 = Dense(self.att_units)\n",
    "      self.W2 = Dense(self.att_units)\n",
    "      self.Va = Dense(1)\n",
    "  \n",
    "  \n",
    "  def call(self,decoder_hidden_state,encoder_output):\n",
    "    if self.scoring_function == 'dot':\n",
    "        decoder_hidden_state = tf.expand_dims(decoder_hidden_state, 2)\n",
    "        score = tf.matmul(encoder_output, decoder_hidden_state)\n",
    "\n",
    "    elif self.scoring_function == 'general':\n",
    "        decoder_hidden_state = tf.expand_dims(decoder_hidden_state, 2)\n",
    "        score = tf.matmul(self.W(encoder_output), decoder_hidden_state)\n",
    "\n",
    "    elif self.scoring_function == 'concat':\n",
    "        decoder_hidden_state = tf.expand_dims(decoder_hidden_state, 1)\n",
    "        score = self.Va(tf.nn.tanh(self.W1(decoder_hidden_state) + self.W2(encoder_output)))\n",
    "\n",
    "    attention_weight = tf.nn.softmax(score, axis=1)\n",
    "    context_vector = tf.reduce_sum(attention_weight * encoder_output, axis=1)\n",
    "    return context_vector, attention_weight\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------\n",
    "class One_Step_Decoder(tf.keras.Model):\n",
    "  def __init__(self, tar_vocab_size, embedding_dim, input_length, dec_units, score_fun, att_units):\n",
    "      super().__init__()\n",
    "      self.tar_vocab_size = tar_vocab_size\n",
    "      self.embedding_dim = embedding_dim\n",
    "      self.input_length = input_length\n",
    "      self.dec_units= dec_units\n",
    "      self.score_fun = score_fun\n",
    "      self.att_units = att_units\n",
    "\n",
    "  def build(self, input_shape):\n",
    "        self.embedding = Embedding(input_dim=self.tar_vocab_size, output_dim=self.embedding_dim, input_length=self.input_length, mask_zero=True, weights=[decoder_embedding_matrix_fast], trainable=False, name=\"Decoder_Embedding\")\n",
    "        self.lstm = LSTM(self.dec_units*2, return_state=True, return_sequences=True, name=\"Decoder_LSTM\")\n",
    "        self.dense = Dense(self.tar_vocab_size)\n",
    "        self.attention = Attention(self.score_fun, self.att_units)\n",
    "\n",
    "  def call(self,input_to_decoder, encoder_output, state_h, state_c):\n",
    "    \n",
    "    target_embedd = self.embedding(input_to_decoder)\n",
    "\n",
    "    context_vector, attention_weights = self.attention(state_h, encoder_output)\n",
    "    context_vector = tf.expand_dims(context_vector, 1)\n",
    "\n",
    "    decoder_input = tf.concat([target_embedd, context_vector], 2)\n",
    "\n",
    "    decoder_output, decoder_final_state_h, decoder_final_state_c = self.lstm(decoder_input, initial_state=[state_h,state_c])\n",
    "\n",
    "    output = self.dense(decoder_output)\n",
    "\n",
    "    return tf.squeeze(output), decoder_final_state_h, decoder_final_state_c, attention_weights, tf.squeeze(context_vector)\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,out_vocab_size, embedding_dim, input_length, dec_units ,score_fun, att_units):\n",
    "      super(Decoder, self).__init__()\n",
    "      self.out_vocab_size = out_vocab_size\n",
    "      self.embedding_dim = embedding_dim\n",
    "      self.input_length = input_length\n",
    "      self.dec_units = dec_units\n",
    "      self.score_fun = score_fun\n",
    "      self.att_units = att_units\n",
    "      self.one_step_decoder = One_Step_Decoder(self.out_vocab_size, self.embedding_dim, self.input_length, self.dec_units, self.score_fun, self.att_units)\n",
    "\n",
    "   \n",
    "    def call(self, input_to_decoder, encoder_output_state, decoder_hidden_state, decoder_cell_state):\n",
    "      all_outputs = tf.TensorArray(tf.float32, size=tf.shape(input_to_decoder)[1], name=\"output_arrays\")\n",
    "\n",
    "      for timestep in range(tf.shape(input_to_decoder)[1]):\n",
    "        output, decoder_hidden_state, decoder_cell_state, attention_weights, context_vector = self.one_step_decoder(input_to_decoder[:,timestep:timestep+1], encoder_output_state, decoder_hidden_state, decoder_cell_state)\n",
    "        all_outputs = all_outputs.write(timestep, output)\n",
    "\n",
    "      all_outputs = tf.transpose(all_outputs.stack(), [1, 0, 2])\n",
    "      return all_outputs\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------\n",
    "class Encoder_Decoder(tf.keras.Model):\n",
    "  def __init__(self, encoder_inputs_length, decoder_inputs_length, input_vocab_size, output_vocab_size, embedding_dim, enc_units, dec_units, att_units, batch_size, score_fun, name='Encoder-Decoder-Attention'):\n",
    "    super().__init__(name=name)\n",
    "    self.encoder = Encoder(in_vocab_size=input_vocab_size+1, embedding_dim=embedding_dim, enc_units=enc_units, input_length=encoder_inputs_length)\n",
    "    self.decoder = Decoder(out_vocab_size=output_vocab_size+1, embedding_dim=embedding_dim, input_length=decoder_inputs_length, dec_units=dec_units, score_fun=score_fun, att_units=att_units)\n",
    "    self.batch_size = batch_size\n",
    "\n",
    "  def call(self,data):\n",
    "\n",
    "    input, output = data[0], data[1]\n",
    "\n",
    "    encoder_output, encoder_h, encoder_c = self.encoder(input)\n",
    "    decoder_output = self.decoder(output, encoder_output, encoder_h, encoder_c)\n",
    "    return decoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ioBubCAmEHWN"
   },
   "source": [
    "## Custom Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QY_3izrXMs8y"
   },
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Bk5pj7kA06y"
   },
   "source": [
    "## Model Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rO1R0QrVn3t3",
    "outputId": "aea541a0-6a54-44b3-fc06-edb0780e35f6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f790cd25d50>"
      ]
     },
     "execution_count": 103,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model  = Encoder_Decoder(encoder_inputs_length=25,\n",
    "                          decoder_inputs_length=25,\n",
    "                          input_vocab_size=vocab_size_input,\n",
    "                          output_vocab_size=vocab_size_output, \n",
    "                          embedding_dim=300, \n",
    "                          enc_units=256, \n",
    "                          dec_units=256, \n",
    "                          att_units=256,\n",
    "                          batch_size=512,\n",
    "                          score_fun='dot')\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),loss=loss_function)\n",
    "\n",
    "model.load_weights(model_path + 'att50/' + 'attention50')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eBrkUqtNT6Pn"
   },
   "source": [
    "## Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MP3kLZoPMvSu"
   },
   "outputs": [],
   "source": [
    "def predict(input_sentence):\n",
    "\n",
    "  batch_size = 1 \n",
    "  DECODER_SEQ_LEN = 25\n",
    "  predict_word_idx = np.zeros((1, 1))\n",
    "  predict_word_idx[0,0] = 1\n",
    "  predicted_sentence = ''\n",
    "\n",
    "  input_sequence=tokenizer_i.texts_to_sequences([input_sentence])\n",
    "  inputs=pad_sequences(input_sequence,maxlen=25,padding='post')\n",
    "  inputs=tf.convert_to_tensor(inputs)\n",
    "\n",
    "  enc_output, state_h, state_c = model.layers[0](inputs)\n",
    "\n",
    "  for i in range(DECODER_SEQ_LEN):\n",
    "      dec_output, state_h, state_c, attention_weights, context_vector = model.layers[1].one_step_decoder(predict_word_idx, enc_output, state_h, state_c)\n",
    "      predict_word_idx = np.reshape(np.argmax(dec_output), (1, 1))\n",
    "      predicted_sentence += ' ' + tokenizer_o.index_word[int(predict_word_idx)]\n",
    "\n",
    "      if tokenizer_o.word_index['<end>'] == predict_word_idx:\n",
    "          return predicted_sentence\n",
    "      dec_input = tf.expand_dims([predict_word_idx],0)\n",
    "\n",
    "  return predicted_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fjT8eN8bOBAO"
   },
   "outputs": [],
   "source": [
    "def final_function_2(input_text, output_text):\n",
    "  input_text = [input_text.split()]\n",
    "  output_text = output_text.split()\n",
    "  bleu_score = bleu.sentence_bleu(input_text, output_text)\n",
    "  return bleu_score\n",
    "\n",
    "def final_function_1(input_list):\n",
    "  output_list = []\n",
    "  bleu_list = []\n",
    "  for i in range(len(input_list)):\n",
    "    input_text = input_list[i]\n",
    "    clean_text = clean(input_text)\n",
    "    output_text = predict(clean_text)\n",
    "    output_text = ' '.join(output_text.split()[:-1])\n",
    "\n",
    "    bleu = final_function_2(input_text, output_text)\n",
    "    bleu_list.append(bleu)\n",
    "    output_list.append(output_text)\n",
    "  return output_list, bleu_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OxGhg_m3NYUJ",
    "outputId": "bdbba0de-0dd0-4d71-ca47-5c9dab7e6b8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text: It is so narrow that I have to keep my body very fit all the time\n",
      "Output Text: It is so narrow that I have to keep my body very all the time\n",
      "BLEU Score: 0.8196501312471536\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "input_list = ['It is so narrow that I have to keep my body very fit all the time']\n",
    "\n",
    "output_list, bleu_list = final_function_1(input_list)\n",
    "\n",
    "for i in range(len(input_list)):\n",
    "  print('Input Text:', input_list[i])\n",
    "  print('Output Text:', output_list[i])\n",
    "  print('BLEU Score:', bleu_list[i])\n",
    "  print('='*80)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "6_Final.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
